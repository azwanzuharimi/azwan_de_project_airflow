{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f2885a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T17:19:19.436915Z",
     "iopub.status.busy": "2024-04-25T17:19:19.436458Z",
     "iopub.status.idle": "2024-04-25T17:19:19.445132Z",
     "shell.execute_reply": "2024-04-25T17:19:19.443959Z"
    },
    "papermill": {
     "duration": 0.01707,
     "end_time": "2024-04-25T17:19:19.446801",
     "exception": false,
     "start_time": "2024-04-25T17:19:19.429731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from snowflake.connector.pandas_tools import pd_writer\n",
    "import snowflake.connector.pandas_tools\n",
    "import snowflake.connector\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "# Load env variable (to get credentials)\n",
    "load_dotenv(dotenv_path=Path().absolute().as_posix().split('working_files')[0]  + '/working_files/dags/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c32b7417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:/Users/AzwanDesktop/OneDrive/Teaching Materials/Invoke/Codes/w4_2_airflow_ETL/Airflow//working_files/data\n"
     ]
    }
   ],
   "source": [
    "# Note: This is for us to able to change where we run this code (either in my PC or Docker environment or from Airflow engine)\n",
    "# In Docker env follows Linux - e.g /usr/local/airflow//working_files/data\n",
    "# In PC follows Windows       - e.g. C:/Users/Azwan/Folder/DOSM/airflow/workingfiles/data/....\n",
    "\n",
    "# In Docker\n",
    "# -----------\n",
    "# If we run manually from inside notebook in Docker, the notebook will be running from           /usr/local/airflow/working_files\n",
    "# But if run from Airflow engine, it will run from entrypoint.sh $AIRFLOW_HOME variable which is /usr/local/airflow\n",
    "# The following is a workaround\n",
    "\n",
    "data_path = Path().absolute().as_posix().split('working_files')[0]  + '/working_files/data'\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec1a0907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load data and merge them from DOSM website\n",
    "def load_df(date_of_file = '2022-01'):\n",
    "    \n",
    "    # Read from DOSM API using pandas - Surveys\n",
    "    df = pd.read_parquet(f'https://storage.data.gov.my/pricecatcher/pricecatcher_{date_of_file}.parquet')\n",
    "    # Convert date column into datetime type\n",
    "    if 'date' in df.columns: df['date'] = pd.to_datetime(df['date'])\n",
    "    print('Number of rows loaded...',len(df))\n",
    "\n",
    "    # Premise\n",
    "    premise = pd.read_parquet('https://storage.data.gov.my/pricecatcher/lookup_premise.parquet')\n",
    "    print(len(premise))\n",
    "    premise.head()\n",
    "\n",
    "    # Items\n",
    "    items = pd.read_parquet('https://storage.data.gov.my/pricecatcher/lookup_item.parquet')\n",
    "    print(len(items))\n",
    "    items.head()\n",
    "\n",
    "    # Combine data\n",
    "    merged_data_premise = df.merge(premise, how = 'left', left_on = 'premise_code', right_on = 'premise_code')\n",
    "    merged_data = merged_data_premise.merge(items, how = 'left', left_on = 'item_code', right_on = 'item_code')\n",
    "\n",
    "    # Only get AEON Subang Jaya data\n",
    "    aeon_subang = merged_data.query('premise_code == 3178')\n",
    "    aeon_subang.loc[:,['etl_time']] = datetime.now().isoformat()\n",
    "\n",
    "    return aeon_subang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc5773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_push_to_sf():\n",
    "    # Read AEON Subang data from parquet file which was saved previously\n",
    "    data_path = Path().absolute().as_posix().split('working_files')[0]  + '/working_files/data'\n",
    "\n",
    "    df_raw = pd.read_parquet(data_path + '/price_catcher_raw/aeon.parquet')\n",
    "    # Connect to Snowflake via its API\n",
    "    ctx = snowflake.connector.connect(  user=       os.environ['USER'],\n",
    "                                        password=   os.environ['PASSWORD'],\n",
    "                                        account=    os.environ['ACCOUNT'],\n",
    "                                        warehouse=  os.environ['WH'],\n",
    "                                        database=   os.environ['DB'],\n",
    "                                        schema=     os.environ['SCHEMA'],\n",
    "                                        role=       os.environ['ROLE'])\n",
    "    \n",
    "    # Write the filtered rows into Snowflake using the same connection credentials previously\n",
    "    snowflake.connector.pandas_tools.write_pandas(\n",
    "        conn = ctx,\n",
    "        df = df_raw,\n",
    "        table_name ='AEON',\n",
    "        database = os.environ['DB'],\n",
    "        schema = os.environ['SCHEMA'], \n",
    "        quote_identifiers=False,\n",
    "        auto_create_table = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c084a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows loaded... 2664434\n",
      "2897\n",
      "757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AzwanDesktop\\AppData\\Local\\Temp\\ipykernel_30988\\570478392.py:16: UserWarning: Pandas Dataframe has non-standard index of type <class 'pandas.core.indexes.base.Index'> which will not be written. Consider changing the index to pd.RangeIndex(start=0,...,step=1) or call reset_index() to keep index as column(s)\n",
      "  snowflake.connector.pandas_tools.write_pandas(\n"
     ]
    }
   ],
   "source": [
    "# Instruction: Write a script to load all data from 2022-01 up until 2024-05 (source: https://open.dosm.gov.my/data-catalogue at PriceCatcher section)\n",
    "# You may use a for-loop, manual code, or be creative to load all of the data\n",
    "\n",
    "\n",
    "# Example, you can save the AEON Subang data to a parquet file then push to Snowflake\n",
    "df = load_df(date_of_file = '2022-01')\n",
    "df.to_parquet(data_path + '/price_catcher_raw/aeon.parquet')\n",
    "read_df_push_to_sf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1.529156,
   "end_time": "2024-04-25T17:19:19.711330",
   "environment_variables": {},
   "exception": null,
   "input_path": "./dags/dag_pricecatcher.ipynb",
   "output_path": "./dags/dag_pricecatcher.ipynb",
   "parameters": {},
   "start_time": "2024-04-25T17:19:18.182174",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
